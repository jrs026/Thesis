\chapter{Related Work}
\label{chap:related_work}

Research in statistical machine translation (SMT) began as large parallel
corpora became available. These corpora include the Canadian Hansards
(French-English parliament proceedings) and the Hong Kong Laws Corpus, among many
others. While these corpora were parallel in the sense that they were created by
directly translating text in one language, they were not sentence aligned. Noise
in the form of missing data or sentences without a $1:1$ correspondence made
alignment a non-trivial problem. This lead to the development of several
approaches for aligning parallel corpora in the early 1990s. We will give an
overview of these approaches in Section \ref{sec:parallel_related}.

In addition to aligning parallel texts, there has also been a considerable
amount of work done on finding parallel sentence pairs in comparable corpora. A
comparable corpus is a multilingual collection of documents which may contain
parallel sentences, but is not completely parallel. This broad definition
includes both weakly aligned data such as timestamped multilingual news feeds,
and Wikipedia articles linked at the document level. Depending on the type of
comparable corpus, different methods may be more or less effective for finding
parallel sentences. We will split our review of comparable corpora mining
methods into two categories. In Section \ref{sec:noisy_related}, we will examine
methods used on closely aligned comparable corpora, and in Section
\ref{sec:nonnoisy_related} we will review work on extracting parallel sentences
from less related multilingual documents.

\section{Parallel Corpus Alignment}
\label{sec:parallel_related}

%\paragraph{Length based alignment}
Perhaps the most well known work on parallel corpus alignment is \citet{Gale91,Gale93}.
The authors described a sentence alignment method based on dynamic programming
which used only sentence length to determine whether or not two sentences were
parallel. This method is widely applicable since it assumes almost no linguistic
knowledge.\footnote{The only bit of information about the language pair required
is a ratio of sentence lengths in characters.} Despite this, it achieved very
high accuracy on a corpus of economic reports from the Union Bank of Switzerland
in English, French and German. \citet{Brown91} had a similar approach, using
only sentence lengths to align parallel corpora, but they measured length in
words rather than characters.

%\paragraph{Cognates}
Even when there is no bilingual lexicon available for a language pair, if the
source and target languages are similar enough it may be possible to use the
surface similarity of words to infer cognates. \citet{Simard93} made use of this
by replacing the length based alignment scoring of \citet{Gale93} with a cognate
based scoring method using a simple method for identifying cognates.
\citet{Church93} made use of cognates with a radically different approach:
creating a dotplot of character $n$-gram matches weighted by inverse frequency,
and then finding an alignment which best matches the dots. While this cognate
based approach was intended to work for similar languages, the authors noted
that even in language pairs like Japanese-English, matches can be found on
technical terms and markup.

%\paragraph{Inferring a bilingual dictionary}
The sentence alignment approach of \citet{Kay93} also used little linguistic
knowledge, though they build a bilingual dictionary from the parallel text to
facilitate alignment. Beginning with an initial set of sentence alignments, they
iteratively update the bilingual dictionary and the sentence alignments in a
manner similar to Viterbi EM, though no explicit probability model is given.
\citet{Chen93} had a similar approach, except he
incorporated the learning of both sentence and word alignments into a
probabilistic model. While this is similar to our work in that there is a
generative story of document pairs used to infer sentence alignments,
\citet{Chen93} used a joint probability distribution of source/target sentence
pairs which must be approximated for efficient inference, and several choices
are made in the inference strategy which assume a strongly monotonic sentence
alignment. Stochastic Viterbi EM is used to find the best sentence alignment.
% TODO: Make sure I'm being fair to Chen93, maybe reword

%\paragraph{Vector-based similarity}
As an alternative method for creating a bilingual dictionary, \citet{Fung94}
built a vector for each source/target word representing how it is distributed in
the parallel corpus. The intuition was that since the alignment between the
source and target data was strongly monotonic, so words that appear in the same
relative positions in the source/target corpora are likely to be translations of
one another.

%\paragraph{Bootstrapping}
\citet{Moore02} builds off of the length based alignment approach of
\citet{Gale93} by adding a bootstrapping step after the initial alignment.
First, a length based sentence alignment is done on the parallel corpus. Then,
the sentences found to be parallel are used to train a word alignment model
(IBM Model 1), and the sentence alignment dynamic program is repeated using
the word alignment scores in addition to length based scores. This bootstrapping
approach is popular in work on mining noisy parallel/comparable corpora (see
Section \ref{sec:comparable_related}).

\section{Comparable Corpus Mining}
\label{sec:comparable_related}

\subsection{Noisy Parallel Corpora}
\label{sec:noisy_related}
The first category of work on comparable corpora mining that we will review is
on noisy parallel data. While even corpora called ``parallel'' contain some
noise, we are refering to corpora which the methods in Section
\ref{sec:parallel_related} would fail on.

Similar to the dynamic programming approaches explored in Section
\ref{sec:parallel_related}, \citet{Zhao02} used a dynamic programming strategy
for aligning parallel sentences in a document pair. They create a probabilistic
model of a comparable document pair $P(S,T,A)$ and choose an alignment to
maximize the probability of the observed source and target documents. To
estimate the probability of two sentences being aligned, they used and IBM-style 
word alignment models (Model 3, specifically) which were estimated on existing
parallel data. \citet{Zhao02} also describes a bootstraping approach where high
confidence sentence alignments are added to the training data for the word
alignment model, and then sentence alignments are recomputed. Much of the work
on noisy parallel/comparable corpora mining used this technique 
\citep{Fung04a,Fung04b,Wu05,Munteanu05}.

\subsection{Comparable Corpora}
\label{sec:nonnoisy_related}
In comparable corpora such as bilingual news feeds or websites, the document
alignment is often not given.\footnote{A notable exception to this is Wikipedia}
First, we will review methods for finding comparable document pairs in a
comparable corpus, and then methods for identifying parallel sentence pairs
within these documents.

\subsubsection{Finding Comparable Document Pairs}
% Doc similarity (Munteanu, Fung)
The Gigaword corpus contains news feeds in multiple languages, and is annotated
with the date of publication. Since these news articles are potentially on the
same topic, there are potentially parallel sentence pairs in these articles.
\citet{Munteanu04,Munteanu05, Fung04a, Fung04b} make use of this information to
find comparable document pairs. The basic strategy is to first consider all
bilingual article pairs published within a time window to be potentially
comparable. Then, documents in one language are projected through a bilingual
dictionary, and bag-of-words based document similarity measures are used to
prune this large set of document pairs. This requires either existing parallel
data or at least a bilingual dictionary. Document pairs that pass through these
filter are then mined for parallel sentences.

% STRAND
Multilingual websites are another potential source for comparable or parallel
document pairs. STRAND \citep{Resnik03} used some heuristics for identifying
links between versions of the same website in different languages. This provides
a candidate set of document pairs, which are further filtered by looking at
their HTML structure. Each website is converted into a list of start tags, end
tags, and ``chunks'' (text within a tag), and these lists are aligned using
standard dynamic programming techniques. This alignment is not only used to
determine whether a pair of websites is comparable, but it also gives an
alignment of text chunks which greatly narrows down the space of possible
sentence alignments

A drastically different approach for finding parallel web pages is given by 
\citet{Uszkoreit10}. Using a existing language identification and translation
systems, they identify the language of all webpages and translate the
non-English ones into English. Since all documents are now in the same language,
the problem of identifying comparable webpages is treated as near-duplicate
detection. An index is built mapping $n$-grams to documents, and this index is
used to find a bag-of-$n$-grams score for potentially comparable documents. The
computation is kept feasible by only creating index entries for rare $n$-grams.

\subsubsection{Finding Parallel Sentences}
Once comparable document pairs have been identified, most comparable corpora
extraction methods will independently judge each sentence pair as parallel or
non-parallel. Since there is often a very large amount of document pairs and
thus potential sentence pairs, filters are used to prune out sentence pairs that
are highly unlikely to be parallel. For example, \citet{Munteanu05} used a
sentence length filter to remove sentence pairs where one sentence was more than
twice as long as the other. In addition, they used a word overlap filter based
on the bilingual dictionary used to find candidate document pairs.

Given a filtered set of sentence pairs, more expensive methods of scoring
sentence pairs can be used. \citet{Munteanu05} use a MaxEnt binary MaxEnt
classifier to ultimately determine whether or not a sentence pair is parallel.
The classifier is trained on parallel data and makes used of features which are
mostly based on word alignments. Others
\cite{Fung04a,Fung04b,Tillmann09a,Tillmann09b} use a single score for sentence
pairs based on either a word alignment model or bag-of-words similarity after
projection through a bilingual lexicon, and tune a threshold on held out data.

% Sub-sentence alignment?
